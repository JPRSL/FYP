{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import copy\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available CUDA devices and memory\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    for i in range(num_devices):\n",
    "        device = torch.cuda.device(i)\n",
    "        total_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3  # Convert to GB\n",
    "        allocated_mem = torch.cuda.memory_allocated(i) / 1024**3  # Convert to GB\n",
    "        free_mem = total_mem - allocated_mem\n",
    "        \n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Total Memory: {total_mem:.1f}GB\")\n",
    "        print(f\"Allocated Memory: {allocated_mem:.1f}GB\")\n",
    "        print(f\"Free Memory: {free_mem:.1f}GB\")\n",
    "        \n",
    "        if free_mem < 8:\n",
    "            print(f\"Warning: GPU {i} has less than 8GB of free VRAM!\")\n",
    "        else:\n",
    "            print(f\"Using GPU {i} with {free_mem:.1f}GB free VRAM\")\n",
    "            break \n",
    "    device = torch.device(f\"cuda:{i}\")\n",
    "else:\n",
    "    print(\"Warning: No CUDA devices available - running on CPU only\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1383b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression dataset\n",
    "class SolarPV(Dataset):\n",
    "    # Initialise dataset\n",
    "    def __init__(self, df, seq_length=64):\n",
    "        self.features = df.drop(columns=['datetime', 'GG']).values.astype('float32')\n",
    "        self.targets = df['GG'].values.astype('float32')\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    # Return length of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_length\n",
    "\n",
    "    # Load and return item from dataset\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx:idx + self.seq_length]\n",
    "        y = self.targets[idx + self.seq_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM regression model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    # x: input tensor of shape (batch_size, seq_length, input_size)\n",
    "    # Returns output tensor of shape (batch_size, output_size)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Get the last time step\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53076148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train local model\n",
    "def train_local(model, dataloader, epochs, lr, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # Define loss function and optimiser\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #model.train()\n",
    "        total_loss = 0.0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            pred = model(x_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "    return model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f189a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate local model parameters\n",
    "def aggregate_params(local_params):\n",
    "    # Assign first local model parameters\n",
    "    avg_params = copy.deepcopy(local_params[0])\n",
    "    # Iterate through each key in parameters\n",
    "    for key in avg_params.keys():\n",
    "        for i in range(1, len(local_params)):\n",
    "            avg_params[key] += local_params[i][key]\n",
    "\n",
    "        # Compute average\n",
    "        avg_params[key] = avg_params[key] / len(local_params)\n",
    "\n",
    "    return avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20831ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_train(global_model, dataloaders, rounds, local_epochs, lr, device):\n",
    "    # FL training loop\n",
    "    for round in range(rounds):\n",
    "        print(f\"--- Federated Learning Round {round + 1}/{rounds} ---\")\n",
    "        local_params = []\n",
    "        \n",
    "        # Train local models and collect parameters\n",
    "        for i, dataloader in enumerate(dataloaders):\n",
    "            print(f\"Training Community {i + 1} local model\")\n",
    "            # Distribute copy of global model to each community\n",
    "            local_model = copy.deepcopy(global_model)\n",
    "            # Train local model on community data\n",
    "            local_param = train_local(local_model, dataloader, local_epochs, lr, device)\n",
    "            # Save and return local model parameters\n",
    "            local_params.append(local_param)\n",
    "        \n",
    "        # Aggregate parameters from all local models\n",
    "        agg_params = aggregate_params(local_params)\n",
    "        \n",
    "        # Update global model with aggregated parameters\n",
    "        global_model.load_state_dict(agg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa92ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FL with final round of community training\n",
    "def fl_train_v2(global_model, dataloaders, rounds, local_epochs, lr, device):\n",
    "    # FL training loop\n",
    "    for round in range(rounds):\n",
    "        print(f\"--- Federated Learning Round {round + 1}/{rounds} ---\")\n",
    "        local_params = []\n",
    "        \n",
    "        # Train local models and collect parameters\n",
    "        for i, dataloader in enumerate(dataloaders):\n",
    "            print(f\"Training Community {i + 1} local model\")\n",
    "            # Distribute copy of global model to each community\n",
    "            local_model = copy.deepcopy(global_model)\n",
    "            # Train local model on community data\n",
    "            local_param = train_local(local_model, dataloader, local_epochs, lr, device)\n",
    "            # Save and return local model parameters\n",
    "            local_params.append(local_param)\n",
    "        \n",
    "        # Aggregate parameters from all local models\n",
    "        agg_params = aggregate_params(local_params)\n",
    "        \n",
    "        # Update global model with aggregated parameters\n",
    "        global_model.load_state_dict(agg_params)\n",
    "    \n",
    "    # Final round of community training\n",
    "    print (\"--- Final Community Training Round ---\")\n",
    "    final_models = []\n",
    "    # Train local models and collect parameters\n",
    "    for i, dataloader in enumerate(dataloaders):\n",
    "        print(f\"Training Community {i + 1} local model\")\n",
    "        # Distribute copy of global model to each community\n",
    "        local_model = copy.deepcopy(global_model)\n",
    "        # Train local model on community data\n",
    "        local_param = train_local(local_model, dataloader, local_epochs, lr, device)\n",
    "        # Save and return local models\n",
    "        final_models.append(local_model)\n",
    "    \n",
    "    return final_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            pred = model(x_batch)\n",
    "\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            targets.extend(y_batch.cpu().numpy())\n",
    "            \n",
    "\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #mape = np.mean(np.abs((np.array(targets) - np.array(preds)) / np.array(targets))) * 100\n",
    "    r2 = r2_score(targets, preds)\n",
    "    print(f\"Evaluation - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2 Score: {r2:.4f}\")\n",
    "    return preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_test(model, dataloaders, scaler_output_list, device):\n",
    "    for i, test_loader in enumerate(dataloaders):\n",
    "        print(f\"Evaluating global model on Community {i + 1} test data\")\n",
    "        #print(f\"Evaluating Community {i + 1} model\")\n",
    "        preds, targets = evaluate(model, test_loader, device)\n",
    "        \n",
    "        # Inverse transform predictions and targets\n",
    "        scaler_output = scaler_output_list[i]\n",
    "        preds_unscaled = scaler_output.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
    "        targets_unscaled = scaler_output.inverse_transform(np.array(targets).reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "        mse = mean_squared_error(targets_unscaled, preds_unscaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "        print(f\"Community {i + 1} Evaluation:\")\n",
    "        print(f\"MAE  (Mean Absolute Error):      {mae:.4f}\")\n",
    "        print(f\"MSE  (Mean Squared Error):       {mse:.4f}\")\n",
    "        print(f\"RMSE (Root Mean Squared Error):  {rmse:.4f}\")\n",
    "        #print(f\"MAPE (Mean Absolute % Error):    {mape:.2f}%\")\n",
    "        print(f\"R²   (Coefficient of Determination): {r2:.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(targets_unscaled[8000:8400], label='Actual', linestyle='-', marker='o', alpha=0.7)\n",
    "        plt.plot(preds_unscaled[8000:8400], label='Predicted', color=\"red\", linestyle='--', marker='x', alpha=0.7)\n",
    "        plt.title(f\"Community {i + 1} Predicted vs Actual Solar Generation\")\n",
    "        plt.xlabel(\"Half-hourly Sample\")\n",
    "        plt.ylabel(\"Solar Generation (kW)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb872a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_test2(model, dataloaders, scaler_output_list, device):\n",
    "    for i, test_loader in enumerate(dataloaders):\n",
    "        print(f\"Evaluating global model on Community {i + 1} test data\")\n",
    "        #print(f\"Evaluating Community {i + 1} model\")\n",
    "        preds, targets = evaluate(model, test_loader, device)\n",
    "        \n",
    "        # Inverse transform predictions and targets\n",
    "        scaler_output = scaler_output_list[i]\n",
    "        preds_unscaled = scaler_output.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n",
    "        targets_unscaled = scaler_output.inverse_transform(np.array(targets).reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "        mse = mean_squared_error(targets_unscaled, preds_unscaled)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "        print(f\"Community {i + 1} Evaluation:\")\n",
    "        print(f\"MAE  (Mean Absolute Error):      {mae:.4f}\")\n",
    "        print(f\"MSE  (Mean Squared Error):       {mse:.4f}\")\n",
    "        print(f\"RMSE (Root Mean Squared Error):  {rmse:.4f}\")\n",
    "        #print(f\"MAPE (Mean Absolute % Error):    {mape:.2f}%\")\n",
    "        print(f\"R²   (Coefficient of Determination): {r2:.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(targets_unscaled[4000:4400], label='Actual', linestyle='-', marker='o', alpha=0.7)\n",
    "        plt.plot(preds_unscaled[4000:4400], label='Predicted', color=\"red\", linestyle='--', marker='x', alpha=0.7)\n",
    "        plt.title(f\"Community {i + 1} Predicted vs Actual Solar Generation\")\n",
    "        plt.xlabel(\"Half-hourly Sample\")\n",
    "        plt.ylabel(\"Solar Generation (kW)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, batch_size=16):\n",
    "    df_train = df[df[\"datetime\"] < \"2012-07-01\"]\n",
    "    df_test = df[df[\"datetime\"] >= \"2012-07-01\"]\n",
    "    # Create a copy of the DataFrames to avoid modifying the original data\n",
    "    df_train = df_train.copy()\n",
    "    df_test = df_test.copy()\n",
    "\n",
    "    numerical_columns = [\"NL\", \"NL_t-24\", \"NL_t-24*7\", \"NL_t-24*30\"]\n",
    "    target_column = [\"GG\"]\n",
    "\n",
    "    # Apply data normalization [0, 1] for numerical columns and target column\n",
    "    scaler_input = MinMaxScaler()\n",
    "    scaler_output = MinMaxScaler()\n",
    "    df_train[numerical_columns] = scaler_input.fit_transform(df_train[numerical_columns])\n",
    "    df_test[numerical_columns] = scaler_input.transform(df_test[numerical_columns])\n",
    "\n",
    "    df_train[target_column] = scaler_output.fit_transform(df_train[target_column])\n",
    "    df_test[target_column] = scaler_output.transform(df_test[target_column])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_dataset = SolarPV(df_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    # Create DataLoader for test data\n",
    "    test_dataset = SolarPV(df_test)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, test_dataloader, scaler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff583e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_list = []\n",
    "test_dataloader_list = []\n",
    "scaler_output_list = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    # Load dataset for each community\n",
    "    df = pd.read_csv(f\"./flmodel_data/community_{i}.csv\")\n",
    "    train_dataloader, test_dataloader, scaler_output = create_dataloaders(df)\n",
    "    \n",
    "    train_dataloader_list.append(train_dataloader)\n",
    "    test_dataloader_list.append(test_dataloader)\n",
    "    scaler_output_list.append(scaler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70603cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, train_loader in enumerate(train_dataloader_list):\n",
    "    X_sample, y_sample = next(iter(train_loader))  # Get first batch\n",
    "    print(f\"Community {i+1}: Feature Shape {X_sample.shape}, Target Shape {y_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, scaler in enumerate(scaler_output_list):\n",
    "    print(f\"Community {i+1}: Scaler Min {scaler.data_min_}, Scaler Max {scaler.data_max_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, train_loader in enumerate(train_dataloader_list):\n",
    "    train_size = len(train_loader.dataset)\n",
    "    test_size = len(test_dataloader_list[i].dataset)\n",
    "    print(f\"Community {i+1}: Train Size {train_size}, Test Size {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global model\n",
    "global_model = LSTMModel(input_size=train_dataloader_list[0].dataset.features.shape[1]).to(device)\n",
    "# Train federated learning model\n",
    "fl_train(global_model, train_dataloader_list, rounds=4, local_epochs=5, lr=0.001, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab25e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate global model on each community's test data\n",
    "fl_test(global_model, test_dataloader_list, scaler_output_list, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc50195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global model\n",
    "global_model_v2 = LSTMModel(input_size=train_dataloader_list[0].dataset.features.shape[1]).to(device)\n",
    "# Train federated learning model\n",
    "final_models = fl_train_v2(global_model_v2, train_dataloader_list, rounds=3, local_epochs=5, lr=0.001, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68843a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_test_v2(final_models, test_dataloader_list, scaler_output_list, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
